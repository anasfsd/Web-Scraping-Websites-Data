from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
import time

# Set up the Chrome driver
service = Service(executable_path='D:/chromedriver.exe')  # Ensure the path is correct
options = webdriver.ChromeOptions()
driver = webdriver.Chrome(service=service, options=options)
driver.get("https://chromewebstore.google.com/detail/free-vpn-for-chrome-vpn-p/majdfhpaihoncoakbjgbdhglocklcgno?hl=en")
time.sleep(50)

try:
    # Open the input and output files
    with open(r"C:\Users\Hassan Laptop Point\Downloads\batteryworld.txt", "r", encoding="utf-8") as file:
        with open(r"C:\Users\Hassan Laptop Point\Downloads\batteryworld2.txt", "w", encoding="utf-8") as file_write:
            lines = file.readlines()  # Read all lines from the input file

            for line in lines:
                url = line.strip().split("####")[0]  # Extract the URL

                driver.get(url)  # Open the URL in the browser
                time.sleep(10)  # Wait for the page to load

                # Get page source and parse with BeautifulSoup
                content = driver.page_source
                soup = BeautifulSoup(content, "html.parser")

                # Find all product list items
                products = soup.find_all("li", class_="type-product")

                # Extract product hrefs
                for product in products:
                    link_tag = product.find("h4", class_="post-title")
                    if link_tag:
                        link_anchor = link_tag.find("a", href=True)
                        if link_anchor:
                            link_href = link_anchor["href"]
                            print(link_href)  # Print output for debugging

                            # Extract breadcrumbs safely
                            try:
                                breadcrumb_div = soup.find("div", class_="et-breadcrumbs")
                                if breadcrumb_div:
                                    breadcrumbs = [element.get_text(strip=True) for element in
                                                   breadcrumb_div.find_all(['a', 'span'])]
                                    breadcrumbs = list(filter(None, breadcrumbs))  # Remove empty elements
                                else:
                                    breadcrumbs = []
                            except Exception as e:
                                print(f"Error extracting breadcrumbs: {e}")
                                breadcrumbs = []
                            print(breadcrumbs)

                            # Write to file (outside the try-except block)
                            file_write.write(link_href + "####" + " > ".join(breadcrumbs) + "####" + line)

finally:
    driver.quit()  # Close the browser session
