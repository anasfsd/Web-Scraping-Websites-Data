from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import time

# Initialize the WebDriver
options = Options()
# options.add_argument("--start-maximized")  # Uncomment if needed
service = Service(executable_path='D:/chromedriver.exe')  # Replace with the path to your ChromeDriver
driver = webdriver.Chrome(service=service, options=options)

# Navigate to the initial URL
# driver.get("https://www.google.com/search?gs_ssp=eJzj4tTP1TcwMU02T1JgNGB0YPBiS8_PT89JBQBASQXT&q=google&oq=g&gs_lcrp=EgZjaHJvbWUqDQgBEC4YxwEY0QMYgAQyDggAEEUYJxg7GIAEGIoFMg0IARAuGMcBGNEDGIAEMgYIAhBFGEAyBwgDEAAYgAQyBwgEEAAYgAQyBwgFEAAYgAQyBggGEEUYPDIGCAcQRRg80gEIMTE4M2owajeoAgCwAgA&sourceid=chrome&ie=UTF-8")
# time.sleep(55)  # Adjust sleep based on network conditions if needed
number=1
try:
    # Open input and output files
    with open(r"C:\Users\Hassan Laptop Point\Downloads\cinar.txt", "r", encoding="utf-8") as file:
        with open(r"C:\Users\Hassan Laptop Point\Downloads\cinar2.txt", "w", encoding="utf-8") as file_write:
            lines = file.readlines()

            # Process each URL in the input file
            for line in lines:
                name = line.strip().split("####")  # Remove newline and split by '####'
                url = name[0]  # Extract URL

                driver.get(url)
                time.sleep(3)  # Allow page load

                # Parse the page source with BeautifulSoup
                content = driver.page_source
                soup = BeautifulSoup(content, "html.parser")

                # Find all 'a' tags within the product-image-container
                links = soup.find_all('a', class_="product__media")

                # Extract the href attributes
                img_hrefs = [link.get('href') for link in links if link.get('href')]

                # Write hrefs to the output file
                for href in img_hrefs:
                    print(href)
                    try:
                        # Extract the breadcrumb container
                        breadcrumb_container = soup.find('ul', class_='breadcrumbs__items')

                        if breadcrumb_container:
                            # Extract breadcrumb items
                            breadcrumb_names = [
                                item.get_text(strip=True) for item in
                                breadcrumb_container.find_all('li', class_='breadcrumbs__item')
                            ]
                        else:
                            breadcrumb_names = []

                    except Exception as e:
                        print(f"Error extracting breadcrumbs: {e}")
                        breadcrumb_names = []

                    # Convert list to string format
                    breadcrumb_str = " > ".join(breadcrumb_names)

                    print(breadcrumb_str)

                    number+=1
                    file_write.write(href + "####" + breadcrumb_str + "####" + line)
finally:
    driver.quit()
